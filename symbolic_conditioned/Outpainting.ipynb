{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182ff9b5-0ccc-4ef6-8436-e75cfb4de65f",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4cdef50-124a-45fd-a3ff-d3f88b5fb3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def build_vocab_from_files(root_dir):\n",
    "    all_tokens = set()\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        for fname in files:\n",
    "            if fname.endswith(\".txt\"):\n",
    "                with open(os.path.join(root, fname), 'r') as f:\n",
    "                    tokens = list(map(int, f.read().strip().split()))\n",
    "                    all_tokens.update(tokens)\n",
    "    sorted_vocab = sorted(all_tokens)\n",
    "    itos = {i: tok for i, tok in enumerate(sorted_vocab)}\n",
    "    stoi = {tok: i for i, tok in itos.items()}\n",
    "    return stoi, itos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519e2581-4e7e-4038-b95c-527afac7c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "\n",
    "class OutpaintingDataset(Dataset):\n",
    "    def __init__(self, token_dir, stoi, context_len=64, target_len=64):\n",
    "        self.file_paths = glob.glob(os.path.join(token_dir, '**/*.txt'), recursive=True)\n",
    "        self.stoi = stoi\n",
    "        self.context_len = context_len\n",
    "        self.target_len = target_len\n",
    "        self.samples = []\n",
    "\n",
    "        for path in self.file_paths:\n",
    "            with open(path) as f:\n",
    "                raw_tokens = list(map(int, f.read().strip().split()))\n",
    "                token_indices = [stoi[t] for t in raw_tokens if t in stoi]\n",
    "                if len(token_indices) >= context_len + target_len:\n",
    "                    for i in range(0, len(token_indices) - context_len - target_len + 1, context_len):\n",
    "                        ctx = token_indices[i:i+context_len]\n",
    "                        tgt = token_indices[i+context_len:i+context_len+target_len]\n",
    "                        self.samples.append((ctx, tgt))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.samples[idx]\n",
    "        return torch.tensor(context), torch.tensor(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ddaa7f-f7df-49f9-b5cc-d245a78fd869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_idx):\n",
    "    contexts, targets = zip(*batch)\n",
    "    context_pad = torch.nn.utils.rnn.pad_sequence(contexts, batch_first=True, padding_value=pad_idx)\n",
    "    target_pad = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=pad_idx)\n",
    "    return context_pad, target_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbc8fcda-ed7d-49a3-9b9b-ecb0133db6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "data_path = \"../data/tokenized\"\n",
    "stoi, itos = build_vocab_from_files(data_path)\n",
    "pad_idx = stoi.get(0, 0)\n",
    "\n",
    "context_len, target_len = 64, 64 # not being passed in as they are same as default vals\n",
    "train_dataset = OutpaintingDataset(data_path, stoi)\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6d3a8-4840-4073-ac6a-e2d0ca871385",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37322b7c-9f70-4c23-98df-4a74d0cef309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MusicOutpaintingTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 1024, d_model))\n",
    "        layer = nn.TransformerDecoderLayer(d_model, nhead)\n",
    "        self.decoder = nn.TransformerDecoder(layer, num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) + self.pos_encoding[:, :src.size(1), :]\n",
    "        tgt = self.embedding(tgt) + self.pos_encoding[:, :tgt.size(1), :]\n",
    "        src, tgt = src.transpose(0, 1), tgt.transpose(0, 1)\n",
    "        out = self.decoder(tgt, memory=src)\n",
    "        return self.fc_out(out.transpose(0, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cfef46-4d3e-4c13-9ce2-51953ab3e1d2",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35797698-0666-4231-a19d-3d0a0b3721a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "batch_size = 8\n",
    "epochs = 10\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda b: collate_fn(b, pad_idx))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda b: collate_fn(b, pad_idx))\n",
    "\n",
    "model = MusicOutpaintingTransformer(vocab_size=len(stoi)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88e01a25-40d9-415e-a7f5-7602b3f4c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for x, y in val_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_input = y[:, :-1]\n",
    "        y_target = y[:, 1:]\n",
    "\n",
    "        out = model(x, y_input)\n",
    "        loss = criterion(out.reshape(-1, out.shape[-1]), y_target.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3944839-8c6f-4874-b1a7-e16c3bb2d285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Training]:  38%|███▊      | 7400/19541 [04:24<07:39, 26.40it/s, loss=0.0602]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 1 [Training]: 100%|██████████| 19541/19541 [11:53<00:00, 27.37it/s, loss=0.0632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Train Loss: 0.1029\n",
      "  Val Loss  : 0.0504\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Training]:  85%|████████▌ | 16624/19541 [10:25<01:52, 26.00it/s, loss=0.0511]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 3 [Training]:  89%|████████▊ | 17341/19541 [10:48<01:22, 26.51it/s, loss=0.0404]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "  Train Loss: 0.0489\n",
      "  Val Loss  : 0.0489\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Training]: 100%|██████████| 19541/19541 [12:27<00:00, 26.13it/s, loss=0.0429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "  Train Loss: 0.0486\n",
      "  Val Loss  : 0.0487\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Training]:  39%|███▉      | 7664/19541 [04:48<07:26, 26.61it/s, loss=0.0569]"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Training]\")\n",
    "\n",
    "    for x, y in train_progress:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_input = y[:, :-1]\n",
    "        y_target = y[:, 1:]\n",
    "\n",
    "        out = model(x, y_input)\n",
    "        loss = criterion(out.reshape(-1, out.shape[-1]), y_target.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        train_progress.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss  : {val_loss:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefb1782-98f6-4b93-9a5c-37ac468709f3",
   "metadata": {},
   "source": [
    "## Outpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e161d4da-5e5c-43af-8c24-a1906d45926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special token IDs\n",
    "PAD_TOKEN_ID = stoi.get(0, 0)\n",
    "BOS_TOKEN_ID = stoi.get(1, 1)\n",
    "EOS_TOKEN_ID = stoi.get(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45259829-d3e1-4801-a589-8e65e88f5dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import stream, note, chord\n",
    "\n",
    "def tokens_to_midi(tokens, output_path):\n",
    "    s = stream.Stream()\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        tok = tokens[i]\n",
    "\n",
    "        # Single Note\n",
    "        if 1000 <= tok < 2000:\n",
    "            pitch_midi = tok - 1000\n",
    "            if i + 1 < len(tokens) and 2000 <= tokens[i + 1] < 3000:\n",
    "                dur = (tokens[i + 1] - 2000) / 4.0\n",
    "                s.append(note.Note(pitch_midi, quarterLength=dur))\n",
    "                i += 2\n",
    "            else:\n",
    "                s.append(note.Note(pitch_midi))\n",
    "                i += 1\n",
    "\n",
    "        # Chord\n",
    "        elif 1000 <= tok < 2000:\n",
    "            pitches = []\n",
    "            while i < len(tokens) and 1000 <= tokens[i] < 2000:\n",
    "                pitches.append(tokens[i] - 1000)\n",
    "                i += 1\n",
    "            if i < len(tokens) and 3000 <= tokens[i] < 4000:\n",
    "                dur = (tokens[i] - 3000) / 4.0\n",
    "                s.append(chord.Chord(pitches, quarterLength=dur))\n",
    "                i += 1\n",
    "            else:\n",
    "                s.append(chord.Chord(pitches))\n",
    "        \n",
    "        # Skip other tokens like PAD, BOS, EOS\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    s.write(\"midi\", fp=output_path)\n",
    "    print(f\"Saved MIDI to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b03f592b-39ab-42bf-91d2-41f18e375207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_continuation(model, context, max_length=64, temperature = 0.75, device=device):\n",
    "    model.eval()\n",
    "    generated = []\n",
    "    input_tgt = torch.tensor([[BOS_TOKEN_ID]], device=device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(context, input_tgt)\n",
    "            probs = torch.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input_tgt = torch.cat([input_tgt, next_token], dim=1)\n",
    "            generated.append(next_token.item())\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f66d8bf-186b-43a4-af03-a93a26737935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_488/3225409777.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_context_tensor = torch.tensor(sample_context, dtype=torch.long, device=device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved MIDI to: generated_sample.mid\n"
     ]
    }
   ],
   "source": [
    "# Pick a random sample from val set\n",
    "sample_context, _ = val_dataset[0]\n",
    "\n",
    "# Generate continuation from a context\n",
    "sample_context_tensor = torch.tensor(sample_context, dtype=torch.long, device=device).unsqueeze(0)\n",
    "generated_indices = generate_continuation(model, sample_context_tensor, max_length=64, device=device)\n",
    "\n",
    "# Convert model output indices back to your original token IDs\n",
    "generated_tokens = [itos[i] for i in generated_indices]\n",
    "\n",
    "# Convert to MIDI and save\n",
    "tokens_to_midi(generated_tokens, \"generated_sample.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8709010-3f37-4b3b-9bb5-18ca80f0e0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
